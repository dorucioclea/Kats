{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kats 206 ML_AR Traditional Machine Learning (LightGBM) forecasting: Elaborate example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will introduce how to use the LightGBM in Kats.  The LightGBM in KATS is designed for time series modeling. \n",
    "\n",
    "1. Overview of LightGBM for Time Series Forecasting  \n",
    "2. Build Your Own LightGBM Model From Scratch  \n",
    "    2.1 Introduction to `LightgbmParams`  \n",
    "    2.2 Forecasting using LightGBM model with `LightgbmTS`  \n",
    "    2.3 Forecasting with Hyper-parameter tunning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We provide two types of tutorial notebooks\n",
    "- **Kats 101**, basic data structure and functionalities in Kats \n",
    "- **Kats 20x**, advanced topics, including advanced forecasting techniques, advanced detection algorithms, `TsFeatures`, meta-learning, global model etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we discuss a more elabota example, with external features and some other characteristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# For Google Colab:\n",
    "!pip install kats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml_ar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLARParams, MLARModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesData\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kats'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kats.consts import TimeSeriesData\n",
    "from kats.models.ml_ar import MLARParams, MLARModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load simulated data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# for c in input_df_orig.columns:\n",
    "#     print(f\"----{c}---\")\n",
    "#     print(input_df_orig[c].value_counts())\n",
    "\n",
    "# min(input_df_orig[\"ticket_dollar\"])\n",
    "# max(input_df_orig[\"ticket_dollar\"])\n",
    "\n",
    "company = [\"AA\", \"UA\", \"SOU\", \"Del\"]\n",
    "direction = [\"departure\", \"arrival\"]\n",
    "\n",
    "ds = pd.Series(\n",
    "    pd.date_range(\n",
    "        \"2021-01-01\",\n",
    "        \"2022-05-01\",\n",
    "        freq=\"D\",\n",
    "    )\n",
    ")\n",
    "\n",
    "terminal = []\n",
    "for i in range(20):\n",
    "    terminal.append(''.join(random.choices(string.ascii_uppercase + string.digits, k=3)))\n",
    "\n",
    "k_people = np.random.randint(10000, high=100000, size=len(terminal)*len(company)*len(direction)*len(ds), dtype=int)/1000\n",
    "ticket_dollar = np.random.randint(6600, high=36600, size=len(terminal)*len(company)*len(direction)*len(ds), dtype=int)/100\n",
    "\n",
    "import itertools\n",
    "\n",
    "all_combinations = list(itertools.product(ds, terminal, company, direction))\n",
    "input_df_orig = pd.DataFrame(all_combinations, columns=['ds','terminal', 'company', 'direction'])\n",
    "input_df_orig[\"k_people\"] = k_people\n",
    "input_df_orig[\"ticket_dollar\"] = ticket_dollar\n",
    "input_df_orig.set_index([\"ds\"], inplace=True)\n",
    "\n",
    "input_df = (\n",
    "    input_df_orig.groupby(\n",
    "        [\"terminal\", \"direction\", \"company\"]\n",
    "    )\n",
    "    .resample(\"W\")[[\"k_people\", \"ticket_dollar\"]]\n",
    "    .quantile(q=0.99)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "input_df_orig\n",
    "input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map categorical feature columns into a numerical type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cat = sorted(input_df[\"terminal\"].unique())\n",
    "density_map = dict(zip(unique_cat, range(len(unique_cat))))\n",
    "input_df['terminal_n'] = input_df['terminal'].map(density_map)\n",
    "\n",
    "unique_cat = sorted(input_df[\"direction\"].unique())\n",
    "density_map = dict(zip(unique_cat, range(len(unique_cat))))\n",
    "input_df['direction_n'] = input_df['direction'].map(density_map)\n",
    "\n",
    "unique_cat = sorted(input_df[\"company\"].unique())\n",
    "density_map = dict(zip(unique_cat, range(len(unique_cat))))\n",
    "input_df['company_n'] = input_df['company'].map(density_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preprocessing the dataframe into a dictionary of KATS time series data, we can variables, feature list and categorical features:\n",
    "\n",
    "* target variable(s): [\"ticket_dollar\"] \n",
    "* feature(s): [\"k_people\"]\n",
    "* categorical(s): [\"terminal\", \"direction\", \"company\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"ticket_dollar\"] # can be multible variables\n",
    "feature_list = [\"k_people\"]\n",
    "categoricals = [\"terminal_n\", \"direction_n\", \"company_n\"]\n",
    "# dictionary of Kats Time Series Data\n",
    "grouped = input_df.groupby([\"terminal\", \"direction\", \"company\"])[[\"ds\"] + variables + categoricals + feature_list]\n",
    "data = [] # list of time series \n",
    "data2 = {} # dictionary of time series \n",
    "for name, group in grouped:\n",
    "    data.append(TimeSeriesData(\n",
    "        group[[\"ds\"] + variables + feature_list + categoricals],\n",
    "        time_col_name=\"ds\",\n",
    "    ))\n",
    "    data2[name] = TimeSeriesData(\n",
    "        group[[\"ds\"] + variables + feature_list + categoricals],\n",
    "        time_col_name=\"ds\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we initialize a `MLARParams` instance that we will use \n",
    "to train a weekly model without any seasonality assumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_params = MLARParams(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"quantile\",\n",
    "    target_variable=variables,\n",
    "    horizon=2,\n",
    "    input_window=5,\n",
    "    freq=\"W\",\n",
    "\n",
    "    cov_history_input_windows={\"k_people\":5},\n",
    "    categoricals=categoricals,\n",
    "\n",
    "    n_estimators = 300,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 1.3,\n",
    "    min_split_gain = 0.0181,\n",
    "    num_leaves = 160\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Forecasting using LightGBM model with `MLAR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize an MLAR object as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLARModel(lightgbm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the `MLAR` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time=time.time()\n",
    "model.train(data)\n",
    "end_time=time.time()\n",
    "\n",
    "mod_name = \"Model 1\"\n",
    "run_time = end_time-start_time\n",
    "print('Model: {} run time: {} seconds'.format(mod_name, run_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate prediction based on prediction steps. Note that we cannot generate forecast horizon larger than the `horizon` parameters we initialized in `LightgbmParams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.predict(steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_data_in` in model object is how regressors, categorical features and multiple target variables are input into feature space. Note that in the data space, we enlarge the feature space with several other important features such as calendar features to capture seasonality, lags, `min`, `max`, `mean` (for target variable time series) to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_data.head()\n",
    "\n",
    "train_data_in = pd.DataFrame(model.train_data_in)\n",
    "train_data_in.columns = model.feature_columns\n",
    "\n",
    "train_data_in.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can check forecasting results by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecasting results \n",
    "len(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "non_interactive_plotting = True\n",
    "\n",
    "plotfile = 'ans.pdf'\n",
    "\n",
    "with PdfPages(plotfile) as pdf:\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        curr_data = data[i][\"ticket_dollar\"].to_dataframe().rename(columns = {\"ds\" : \"time\"}).set_index(\"time\")\n",
    "        curr_fc = forecast[i][[\"time\", \"forecast\"]].set_index(\"time\")\n",
    "\n",
    "        plot_df = curr_data.join(curr_fc, how=\"outer\")\n",
    "\n",
    "        plt.figure()\n",
    "        plot_df.plot(figsize=(50,20), title=f\"Series: {i}\")\n",
    "        plt.show()\n",
    "\n",
    "        if non_interactive_plotting:\n",
    "            #plt.savefig(plotfile())\n",
    "            pdf.savefig()\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train process for dictionary of time series \n",
    "\n",
    "model = MLARModel(lightgbm_params)\n",
    "model.train(data2)\n",
    "forecast = model.predict(steps=2)\n",
    "\n",
    "model.train_data.head()\n",
    "\n",
    "model.train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Forecasting with Hyper-parameter tunning \n",
    "As other KATS models, we can use time_series_parameter_tuning for Hyper-parameter Tunning. Specifically, we initialize an evaluation_function with `MLARParams` object for `MLAR` object. In the parameters setting, the  `n_estimators`, `max_depth`, `learning_rate`, `min_split_gain`, `num_leaves`, `num_leaves` are setted up for Hyper-parameter tunning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kats.utils.time_series_parameter_tuning as tpt\n",
    "tpt.SearchMethodFactory.create_search_method\n",
    "# from hyperopt.early_stop import no_progress_loss\n",
    "from kats.consts import TimeSeriesData, SearchMethodEnum\n",
    "from kats.utils.parameter_tuning_utils import get_default_lightgbm_parameter_search_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize a GMEnsemble object as follows. In the function, we can initialize LightgbmParams with parameters not in searching space, otherwise, they will be initialized with default values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(params):\n",
    "#     try:\n",
    "    lightgbm_params = MLARParams(\n",
    "        boosting_type = \"gbdt\",\n",
    "        objective = \"quantile\", \n",
    "        target_variable = variables,\n",
    "        horizon = 3,\n",
    "        input_window = 7,\n",
    "        freq=\"W\",\n",
    "        #expand_feature_space=feature_list,\n",
    "        cov_history_input_windows={\"k_people\":5},\n",
    "        categoricals = categoricals,   \n",
    "        calculate_fit = True,  \n",
    "        #missing_threshold = 0,\n",
    "        #num_missing_to_drop = 100,\n",
    "        **params)\n",
    "    \n",
    "    model = MLARModel(lightgbm_params)\n",
    "    model.train(data)\n",
    "    model._predict()\n",
    "\n",
    "    print(model.train_data)\n",
    "    error = np.mean(np.abs(model.train_data['output'].values - model.train_data[\"forecast\"].values))\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `tpt.SearchMethodFactory.create_search_method` to initialize search algorithm, such as random search, and the bootstrap_size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: replace get_default_lightgbm_parameter_search_space with get_parameter_search_space from ml_ar\n",
    "\n",
    "from kats.utils.parameter_tuning_utils import (\n",
    "    get_default_lightgbm_parameter_search_space,\n",
    ")\n",
    "\n",
    "parameter_tuner = tpt.SearchMethodFactory.create_search_method(\n",
    "    objective_name=\"hpt_example\",\n",
    "    parameters=get_default_lightgbm_parameter_search_space(),  # using default parameter space\n",
    "    selected_search_method=SearchMethodEnum.RANDOM_SEARCH_SOBOL,\n",
    "    evaluation_function=evaluation_function, \n",
    "    bootstrap_size=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_tuner.list_parameter_value_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arm count is used in search strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_tuner.generate_evaluate_new_parameter_values(\n",
    "    evaluation_function=evaluation_function, arm_count=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the parameters randomly selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_tuner.list_parameter_value_scores()\n",
    "\n",
    "#parameter_tuner.list_parameter_value_scores()[\"parameters\"][3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Categorical Feature Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"ticket_dollar\"] # can be multible variables\n",
    "feature_list = [\"k_people\"]\n",
    "categoricals = []\n",
    "# dictionary of Kats Time Series Data\n",
    "grouped = input_df.groupby([\"terminal\", \"direction\", \"company\"])[[\"ds\"] + variables + feature_list]\n",
    "data = []\n",
    "for name, group in grouped:\n",
    "    data.append(TimeSeriesData(\n",
    "        group[[\"ds\"] + variables + feature_list],\n",
    "        time_col_name=\"ds\",\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_params = MLARParams(\n",
    "    boosting_type=\"gbdt\",\n",
    "    objective=\"quantile\",\n",
    "    target_variable=variables,\n",
    "    horizon=2,\n",
    "    input_window=5,\n",
    "    freq=\"W\",\n",
    "    cov_history_input_windows={\"k_people\":5},\n",
    "    categoricals=categoricals,\n",
    "    #missing_threshold=0,\n",
    "    #num_missing_to_drop=10,\n",
    "    n_estimators = 300,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 1.3,\n",
    "    min_split_gain = 0.0181,\n",
    "    num_leaves = 160,\n",
    "    calculate_fit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLARModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMLARModel\u001b[49m(lightgbm_params)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(data)\n\u001b[1;32m      5\u001b[0m forecast \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLARModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = MLARModel(lightgbm_params)\n",
    "\n",
    "model.train(data)\n",
    "\n",
    "forecast = model.predict(steps=2)\n",
    "\n",
    "model.train_data.head()\n",
    "\n",
    "# forecasting results \n",
    "len(forecast)\n",
    "\n",
    "forecast[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
