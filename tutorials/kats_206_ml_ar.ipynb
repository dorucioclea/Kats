{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kats 206 ML_AR Traditional Machine Learning (LightGBM) forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will introduce how to use the autoregressive machine learning model (MLAR) in Kats. It implements a wrapper for LightGBM, performing feature engineering, global modelling, dierct modelling for many horizons and variables at once, and other things. The table of contents for Kats 206 is as follows:\n",
    "\n",
    "TODO: Need to change this\n",
    "1. Overview of Global Model for Forecasting  \n",
    "2. Build Your Own Global Model or Global Ensemble From Scratch  \n",
    "    2.1 Introduction to `GMParam`  \n",
    "    2.2 Forecasting using a single global model with `GMModel`  \n",
    "    2.3 Forecasting using a global model ensemble with `GMEnsemble`  \n",
    "    2.4 Backtesting with `GMBacktester` \n",
    "3. Using Pretrained Global Model or Global Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We provide two types of tutorial notebooks\n",
    "- **Kats 101**, basic data structure and functionalities in Kats \n",
    "- **Kats 20x**, advanced topics, including advanced forecasting techniques, advanced detection algorithms, `TsFeatures`, meta-learning, global model etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of the model\n",
    "\n",
    "The model uses LightGBM, a Gradient-Boosted Tree that got highly popular in forecasting after being used by most of the top entries into the M5 competition. Our wrapper has the following characteristics:\n",
    "\n",
    "### Features:\n",
    "\n",
    "- Lags (input window)\n",
    "- Calendar features and/or Fourier terms\n",
    "- Past covariates and lags of those covariates\n",
    "- Future covariates and lags and future values of those\n",
    "- Summary statistics over the input window: min, max, mean, median, sd \n",
    "\n",
    "### Normalization:\n",
    "\n",
    "It does per-window normalization, to allow for trend modeling. The normalizer can be computed over the full input window or a smaller portion, e.g., only the last 2-3 observations. It has the following options implemented:\n",
    "- Mean, Median, Max, Std over the window\n",
    "- Either divide by those or subtract\n",
    "- Do Z-score per-window normalization, i.e., subtract mean and divide by std.\n",
    "\n",
    "### Direct forecasting and modeling of more than one target:\n",
    "\n",
    "You can specify directly the horizons you want to forecast for. Those are modeled internally as a feature, i.e., you have an input feature that tells you which horizon you want. That way, you can train the model for different horizons at the same time. In a similar way, we even allow to train for different targets at the same time, which can be used, e.g., to have one model that predicts different quantiles. Here, the past covariates and features stay the same across the horizons, so are effectively copied for each horizon, and the future covariates and features (calendar features) are different for each target. As the input matrix is effectively copied for each horizon, having many direct horizons can be memory intensive.\n",
    "\n",
    "### Iterative forecasting:\n",
    "\n",
    "The model also allows for iterating out the forecast. Thus, you can, for example, train a model for direct forecasting of a horizon of 30, and then iterate out to a horizon of 90. This will be done in chunks of 30. Note that for iterative forecasting to work, future values of both past and future covariates are needed, so if your model has past covariates, you need to forecast those for the iterative forecasting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to `MLARParams`\n",
    "\n",
    "TODO: Need to change this\n",
    "\n",
    "All parameters for a GM or a GME are specified using the `GMParam` class.  The `GMParam` class does basic parameter checking when initialized to ensure that the parameters are correctly specified.  Here are some of its key arguments:\n",
    "\n",
    "* **freq**: `str` or `pd.Timedelta`, The time granularity of the model (and the input time series.) For example, `freq='D'` indicates a daily model;\n",
    "* **model_type**: `str`, The name of neural network type - either 'rnn' (recurrent neural network) or 's2s' (sequence to sequence). Default is 'rnn';\n",
    "* **seasonality**: `int`, The integer length of the seasonality period. The default value is 1, indicating a non-seasonal model;\n",
    "* **input_window**: `int`, This parameter specifies the size into which we segment our input time series to feed them into the neural network.  It should be greater than the `seasonality` argument;\n",
    "* **fcst_window**: `int`,  The number of data points forecast in a single forecast step;\n",
    "* **quantile**: `list[float]`, The float values of the quantiles to forecast.  The first value of this list should always be 0.5, representing the median.  The default value is `[0.5,0.05,0.95,0.99]`;\n",
    "* **nn_structure**: `list[list[int]]`, The structure of the neural network. If not specified, the default value is `[[1,3]]`;\n",
    "* **loss_function**: `list[str]`, The name of loss function - either 'pinball' or 'adjustedpinball';\n",
    "* **gmfeature**: `list[str]` or `str`; A single or a list of feature names.\n",
    "\n",
    "For the definition of other parameters, please see our documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A simple example\n",
    "\n",
    "TODO: Need to change this\n",
    "\n",
    "The `GMModel` is our basic class to build a single GM.  Kats also supports Global Model Ensembles (GMEs), which are ensembles of independent GMs, with the `GMEnsemble` class.  The `GMParam` is the parameter class for both `GMModel` and `GMEnsemble`.  We also provide the `GMBacktester` class for parameter tunning and backtesting. \n",
    "\n",
    "The examples in this section are designed to display the basic functionality of each of the aforementioned classes.  They are of limited scale and not expected to provide good performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# For Google Colab:\n",
    "!pip install kats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml_ar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLARParams, MLARModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesData\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kats'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kats.models.ml_ar import MLARParams, MLARModel\n",
    "from kats.consts import TimeSeriesData\n",
    "from kats.utils.simulator import Simulator\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(358749)\n",
    "all_series = {}\n",
    "\n",
    "for i in range(20):\n",
    "    sim = Simulator(n=1000, freq=\"1D\", start = pd.to_datetime(\"2017-01-01\"))\n",
    "    sim.add_trend(magnitude=50)\n",
    "    sim.add_seasonality(5, period=timedelta(days=7))\n",
    "    sim.add_seasonality(10, period=timedelta(days=365))\n",
    "    sim.add_noise(magnitude=2)\n",
    "    sim_ts = sim.stl_sim().to_dataframe()\n",
    "    sim_ts.set_index(\"time\", drop=True, inplace=True)\n",
    "    all_series[f\"country_{i}\"] = sim_ts\n",
    "\n",
    "in_data = pd.concat(all_series, axis=1)\n",
    "\n",
    "in_data.columns = all_series.keys()\n",
    "countries = in_data.columns\n",
    "\n",
    "in_data = in_data.reset_index()\n",
    "in_data.set_index(\"time\", inplace=True, drop=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set LGBM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = \"y\"\n",
    "\n",
    "horizon = 60\n",
    "\n",
    "#horizon = [1,2,3,4,5, 30, 60, 90]\n",
    "\n",
    "input_window = 90\n",
    "\n",
    "lightgbm_params = {}\n",
    "\n",
    "# we use Fourier terms as calendar features and subtract the window mean for normalization.\n",
    "lightgbm_params[\"orig\"] = MLARParams(\n",
    "    n_jobs = 20,\n",
    "    n_estimators = 400,\n",
    "    objective=\"mae\",\n",
    "\n",
    "    target_variable=[target_var],\n",
    "    horizon=horizon,\n",
    "    input_window=input_window,\n",
    "    #freq=\"D\",\n",
    "\n",
    "    num_leaves = 128,\n",
    "    verbose=3,\n",
    "    norm_sum_stat = \"mean\",\n",
    "    sub_div = \"sub\",\n",
    "    calendar_features = []\n",
    "    #fourier_features_order = [],\n",
    "    #fourier_features_period = []\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition into training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2017-01-01\"\n",
    "start_date = pd.to_datetime(start_date)\n",
    "\n",
    "\n",
    "fc_origin = \"2019-09-30\"\n",
    "fc_origin = pd.to_datetime(fc_origin)\n",
    "\n",
    "fc_hor1 = fc_origin + pd.DateOffset(days=1)\n",
    "fc_hor365 = fc_origin + pd.DateOffset(days=365)\n",
    "\n",
    "train_data = in_data[(in_data[\"time\"] >= start_date) & (in_data[\"time\"] <= fc_origin)]\n",
    "test_data = in_data[(in_data[\"time\"] >= fc_hor1) & (in_data[\"time\"] <= fc_hor365)]\n",
    "\n",
    "train_data_dict = {}\n",
    "\n",
    "for curr_country in countries:\n",
    "    curr_data = train_data[[\"time\", curr_country]].rename(columns={curr_country : \"y\"})\n",
    "    train_data_dict[curr_country] = TimeSeriesData(curr_data, time_col_name=\"time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models from parameter configs\n",
    "models = dict(map(lambda x: (x[0], MLARModel(x[1])), lightgbm_params.items()))\n",
    "\n",
    "\n",
    "all_run_times = {}\n",
    "all_forecasts = {}\n",
    "\n",
    "all_run_times[fc_origin] = {}\n",
    "all_forecasts[fc_origin] = {}\n",
    "\n",
    "for mod_name, model in models.items():\n",
    "    start_time=time.time()\n",
    "    model.train(train_data_dict)\n",
    "    end_time=time.time()\n",
    "    all_run_times[fc_origin][mod_name] = end_time-start_time\n",
    "    print('Model: {} run time: {} seconds'.format(mod_name, all_run_times[fc_origin][mod_name]))\n",
    "\n",
    "    # get forecasts\n",
    "    all_forecasts[fc_origin][mod_name] = model.predict(steps=300)\n",
    "\n",
    "\n",
    "all_run_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the forecasts from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = all_forecasts[fc_origin][\"orig\"]\n",
    "\n",
    "for country in forecasts.keys():\n",
    "\n",
    "    #fc = forecasts[country]\n",
    "    fc = forecasts[country][[\"time\", \"forecast\"]].set_index(\"time\")\n",
    "\n",
    "    plot_df = fc.join(train_data[country], how=\"outer\").join(test_data[country], how=\"outer\", lsuffix=\"_train\")\n",
    "\n",
    "    plt.figure()\n",
    "    plot_df.plot(figsize=(50,20), title=f\"Country: {country}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b6e8fba36db23bc4d54e0302cd75fdd75c29d9edcbab68d6cfc74e7e4b30305"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
